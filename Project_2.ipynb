{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dtuimldmtools as dtu\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "seeds_dataset = \"seeds_dataset.txt\"\n",
    "dataset_file = data_path + seeds_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 8)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(dataset_file)\n",
    "# Validate shape of the dataset, 210 rows with 8 attributes\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['area_A',\n",
       "  'perimeter_P',\n",
       "  'compactness_C',\n",
       "  'length_of_kernel',\n",
       "  'width_of_kernel',\n",
       "  'asymmetry_coefficient',\n",
       "  'length_of_kernel_groove',\n",
       "  'class'],\n",
       " 210,\n",
       " 8,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 3., 3.]),\n",
       " (210,),\n",
       " ['Kama', 'Rosa', 'Canadian'],\n",
       " 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data\n",
    "# attributeNames are not present in the dataset, just gonna hardcode based on the website\n",
    "attributeNames = [\n",
    "    \"area_A\",\n",
    "    \"perimeter_P\",\n",
    "    \"compactness_C\",\n",
    "    \"length_of_kernel\",\n",
    "    \"width_of_kernel\",\n",
    "    \"asymmetry_coefficient\",\n",
    "    \"length_of_kernel_groove\",\n",
    "    \"class\",\n",
    "]\n",
    "N = data.shape[0]\n",
    "M = data.shape[1]\n",
    "y = X[:, -1]\n",
    "# This is derived from the website\n",
    "classNames = [\"Kama\", \"Rosa\", \"Canadian\"]\n",
    "C = len(classNames)\n",
    "attributeNames, N, M, y, y.shape, classNames, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure zero-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210, 8),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2.]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, -1] -= 1\n",
    "X.shape, X[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outlier as shown from project_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, (209, 8), (209,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_index = attributeNames.index(\"length_of_kernel\")\n",
    "lowest_index = np.argmin(X[:, 3])\n",
    "X_updated = np.delete(X, lowest_index, axis=0)\n",
    "y = np.delete(y, lowest_index, axis=0)\n",
    "N -= 1\n",
    "N, X_updated.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove class column because we would not be using it for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 7)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_updated = X_updated[:, :-1]\n",
    "X_updated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data\n",
    "Data standardization/ data scaling needs to be done if the data have huge or scattered values, machine learning model needs smaller and coherent values. Data scaling, standardize values in the data set for better results.\"\n",
    "\n",
    "https://www.kaggle.com/discussions/questions-and-answers/159183#910328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X_mean = np.mean(X_updated, axis=0)\n",
    "X_std = np.std(X_updated, axis=0)\n",
    "X_standardized = (X_updated - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((209, 7), (209,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_standardized.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass ANN\n",
    "Since we have three distinct classes: Kama, Rosa and Canadian, we adopt a multiclass approach. \n",
    "As complexity-controlling parameter\n",
    "for the ANN, we will use the number of hidden units3 h. Based on a few test-runs, select\n",
    "a reasonable range of values for h (which should include h = 1), and describe the range of\n",
    "values you will use for h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn import model_selection\n",
    "from dtuimldmtools import dbplotf, train_neural_net, visualize_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 28 11:23:56 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.216.03             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    On  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   58C    P3              19W / 115W |    218MiB /  6144MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3389      G   /usr/lib/xorg/Xorg                          100MiB |\n",
      "|    0   N/A  N/A      8494      C   ...e/monkescripts/anaconda3/bin/python      110MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "CV = model_selection.KFold(K, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.6951456\t0.0004845623\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.7860767\t0.00039678888\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.73495036\t0.000825648\n",
      "\n",
      "\tBest loss: 0.6951456069946289\n",
      "\n",
      "Number of miss-classifications for ANN:\n",
      "\t 3 out of 42. Overall error_rate 0.0714285746216774\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.6804271\t0.0005750186\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.73594654\t0.0016965396\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.7228741\t0.00088009244\n",
      "\n",
      "\tBest loss: 0.680427074432373\n",
      "\n",
      "Number of miss-classifications for ANN:\n",
      "\t 5 out of 42. Overall error_rate 0.1190476194024086\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.7979675\t0.0021439374\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.7391125\t0.0016548043\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.694078\t0.0006035156\n",
      "\n",
      "\tBest loss: 0.6940780282020569\n",
      "\n",
      "Number of miss-classifications for ANN:\n",
      "\t 5 out of 42. Overall error_rate 0.1190476194024086\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.68743557\t0.00070234353\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.7002245\t0.0007841261\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.6938485\t0.0005413341\n",
      "\n",
      "\tBest loss: 0.6874355673789978\n",
      "\n",
      "Number of miss-classifications for ANN:\n",
      "\t 8 out of 42. Overall error_rate 0.190476194024086\n",
      "\n",
      "\tReplicate: 1/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.67158663\t0.00034530344\n",
      "\n",
      "\tReplicate: 2/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.69355804\t0.00052533526\n",
      "\n",
      "\tReplicate: 3/3\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t300\t0.81158036\t0.0005282878\n",
      "\n",
      "\tBest loss: 0.6715866327285767\n",
      "\n",
      "Number of miss-classifications for ANN:\n",
      "\t 4 out of 41. Overall error_rate 0.09756097197532654\n"
     ]
    }
   ],
   "source": [
    "error_rates = []\n",
    "n_hidden_units = 8\n",
    "for train_index, test_index in CV.split(X_standardized, y):\n",
    "    X_train = torch.from_numpy(X_standardized[train_index]).type(torch.float)\n",
    "    y_train = torch.from_numpy(y[train_index]).type(torch.long)\n",
    "    X_test = torch.from_numpy(X_standardized[test_index]).type(torch.float)\n",
    "    y_test = torch.from_numpy(y[test_index]).type(torch.long)\n",
    "    # X_train = torch.from_numpy(X[train_index]).type(torch.float).to(device)\n",
    "    # y_train = torch.from_numpy(y[train_index]).type(torch.long).to(device)\n",
    "    # X_test = torch.from_numpy(X[test_index]).type(torch.float).to(device)\n",
    "    # y_test = torch.from_numpy(y[test_index]).type(torch.long).to(device)\n",
    "    # Recall that last column represents the classes and should not be used as an input feature\n",
    "    input_features = M - 1\n",
    "    num_epochs = 300\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    seed_model = lambda: torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_features, n_hidden_units),  \n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(n_hidden_units, C),\n",
    "        torch.nn.Softmax(dim=1)\n",
    "    )\n",
    "    net, final_loss, learning_curve = train_neural_net(\n",
    "        seed_model,\n",
    "        loss_fn,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        n_replicates=3,\n",
    "        max_iter=num_epochs,\n",
    "    )\n",
    "    print(\"\\n\\tBest loss: {}\\n\".format(final_loss))\n",
    "\n",
    "    # Determine probability of each class using trained network\n",
    "    softmax_logits = net(X_test)\n",
    "    # convert to label with the highest probability\n",
    "    y_preds = torch.argmax(softmax_logits, dim=1)\n",
    "    # Compare error against ground truth y_test\n",
    "    e = y_preds != y_test\n",
    "    error_rate = sum(e) / len(e)\n",
    "    error_rates.append(error_rate)\n",
    "    print(\n",
    "        f\"Number of miss-classifications for ANN:\\n\\t {sum(e)} out of {len(e)}. Overall error_rate {error_rate}\")\n",
    "mean_error_rate = np.mean(np.asarray(error_rates))\n",
    "print(f\"mean_error_rate for {n_hidden_units} hidden_units is {mean_error_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with two level cross validation\n",
    "# TODO add baseline model and workflow as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
